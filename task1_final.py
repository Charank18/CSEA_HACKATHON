# -*- coding: utf-8 -*-
"""task1_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xqqrCf5uF-y8g8ZIvXtMyUnfJkBlWUm-
"""

import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq
import torch
from torch.utils.data import Dataset

df = pd.read_csv("final_labels.csv")

df = df[["body", "highlight"]]
df = df.rename(columns={"body": "input", "highlight": "target"})

print(df.isnull().sum())
df = df.dropna(subset=["input", "target"])
df["input"] = df["input"].astype(str)
df["target"] = df["target"].astype(str)

model_name = "facebook/bart-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def tokenize_data(examples):
    inputs = tokenizer(examples["input"], max_length=512, truncation=True, padding="max_length")

    with tokenizer.as_target_tokenizer():
        outputs = tokenizer(examples["target"], max_length=128, truncation=True, padding="max_length")

    return {
        "input_ids": inputs["input_ids"],
        "attention_mask": inputs["attention_mask"],
        "labels": outputs["input_ids"],
    }

class SummarizationDataset(Dataset):
    def __init__(self, df):
        self.examples = df
        self.tokenized = [tokenize_data({"input": input_text, "target": target_text})
                          for input_text, target_text in zip(df["input"], df["target"])]

    def __len__(self):
        return len(self.tokenized)

    def __getitem__(self, idx):
        item = {
            "input_ids": torch.tensor(self.tokenized[idx]["input_ids"]),
            "attention_mask": torch.tensor(self.tokenized[idx]["attention_mask"]),
            "labels": torch.tensor(self.tokenized[idx]["labels"])
        }
        return item

dataset = SummarizationDataset(df)
from torch.utils.data import random_split
train_size = int(0.9 * len(dataset))
eval_size = len(dataset) - train_size
train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])

training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
    logging_dir="./logs",
    logging_steps=100,
    predict_with_generate=True,
    generation_max_length=128,
    generation_num_beams=4,
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()

model.save_pretrained("./fine-tuned-model")
tokenizer.save_pretrained("./fine-tuned-model")

test_text = df["input"].iloc[0]
inputs = tokenizer(test_text, return_tensors="pt", max_length=512, truncation=True)
summary_ids = model.generate(inputs["input_ids"], num_beams=4, max_length=128)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print(f"Original text: {test_text[:100]}...")
print(f"Summary: {summary}")

import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

model_path = "./fine-tuned-model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

def generate_summary(text, max_length=128):
    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(
        inputs["input_ids"],
        num_beams=4,
        min_length=30,
        max_length=max_length,
        early_stopping=True,
        no_repeat_ngram_size=3,
    )
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

dialogues = [
    ("Child: Mom, I read that drinking enough water can make you look younger! Is that true?\n"
     "Parent: Drinking water is great for your skin, but it wonâ€™t turn back time, sweetheart."),
    ("Child: Dad, my friend refuses to drink water. He looks exhausted all the time!\n"
     "Parent: Maybe remind him that staying hydrated can help with energy and skin health.")
]

print("=" * 80)
print("MODEL SUMMARY EVALUATION")
print("=" * 80)

for i, dialogue in enumerate(dialogues, 1):
    generated_summary = generate_summary(dialogue)

    print(f"\nSAMPLE {i}")
    print("-" * 40)
    print(f"ORIGINAL DIALOGUE:\n{dialogue}")
    print("-" * 40)
    print(f"GENERATED SUMMARY:\n{generated_summary}")
    print("=" * 80)

print("\nINTERACTIVE TESTING")
print("Enter text to summarize (type 'exit' to quit):")

while True:
    user_input = input("\nEnter text: ")
    if user_input.lower() == 'exit':
        break

    if len(user_input) < 50:
        print("Please enter a longer text (at least 50 characters)")
        continue

    summary = generate_summary(user_input)
    print("\nSUMMARY:")
    print(summary)

pip install rouge

import pandas as pd
import torch
import numpy as np

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, BertTokenizer

from torch.nn.functional import softmax
from nltk.translate.bleu_score import sentence_bleu
from rouge import Rouge
from scipy.spatial.distance import cosine
from torch.nn import CrossEntropyLoss


df = pd.read_csv("final_labels.csv").dropna(subset=["body", "highlight"])
test_samples = df.sample(5)

rouge = Rouge()

def evaluate_bleu_rouge(reference, generated):
    reference_tokens = [reference.split()]
    generated_tokens = generated.split()
    bleu_score = sentence_bleu(reference_tokens, generated_tokens)
    rouge_scores = rouge.get_scores(generated, reference)[0]
    return bleu_score, rouge_scores

def calculate_perplexity(text):
    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
    loss = outputs.loss.item()
    return np.exp(loss)

bert_model = AutoModel.from_pretrained("bert-base-uncased")
bert_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def get_embedding(text):
    inputs = bert_tokenizer(text, return_tensors="pt", max_length=512, truncation=True, padding="max_length")
    with torch.no_grad():
        outputs = bert_model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

def calculate_similarity(text1, text2):
    emb1 = get_embedding(text1)
    emb2 = get_embedding(text2)
    return 1 - cosine(emb1, emb2)

for i, row in enumerate(test_samples.iterrows(), 1):
    row = row[1]
    original_text = row["body"]
    reference_summary = row["highlight"]
    generated_summary = generate_summary(original_text)
    bleu, rouge_scores = evaluate_bleu_rouge(reference_summary, generated_summary)
    perplexity = calculate_perplexity(generated_summary)
    similarity = calculate_similarity(reference_summary, generated_summary)

    print(f"\nSAMPLE {i}")
    print("-" * 40)
    print(f"BLEU Score: {bleu:.4f}")
    print(f"ROUGE Scores: {rouge_scores}")
    print(f"Perplexity: {perplexity:.4f}")
    print(f"Semantic Similarity: {similarity:.4f}")

! pip install huggingface_hub

from huggingface_hub import login

login()

api = HfApi()
api.create_repo(
    repo_id="Charankarnati18/TASK1",
    repo_type="model",
    private=False
)

api.upload_folder(
    folder_path="/content/fine-tuned-model",
    repo_id="Charankarnati18/TASK1",
    repo_type="model",
    commit_message="Uploading fine-tuned summarization model"
)

